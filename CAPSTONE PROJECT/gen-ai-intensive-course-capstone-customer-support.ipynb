{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-17T15:11:44.395547Z",
     "iopub.status.busy": "2025-04-17T15:11:44.394935Z",
     "iopub.status.idle": "2025-04-17T15:11:50.616564Z",
     "shell.execute_reply": "2025-04-17T15:11:50.616014Z",
     "shell.execute_reply.started": "2025-04-17T15:11:44.395493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers \n",
    "import accelerate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore Customer Support Dataset\n",
    "Load a real-life customer support dataset (e.g., from Kaggle) and perform exploratory data analysis to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:32:08.174918Z",
     "iopub.status.busy": "2025-04-17T15:32:08.174105Z",
     "iopub.status.idle": "2025-04-17T15:32:08.214122Z",
     "shell.execute_reply": "2025-04-17T15:32:08.213444Z",
     "shell.execute_reply.started": "2025-04-17T15:32:08.174891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   query          10 non-null     object\n",
      " 1   true_response  10 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 292.0+ bytes\n",
      "None\n",
      "                                  query  \\\n",
      "0           How do I reset my password?   \n",
      "1           What is your refund policy?   \n",
      "2     Can I change my shipping address?   \n",
      "3          How long does delivery take?   \n",
      "4  Do you offer international shipping?   \n",
      "\n",
      "                                       true_response  \n",
      "0  Click on 'Forgot Password' on the login page t...  \n",
      "1   You can return items within 30 days of purchase.  \n",
      "2  Yes, you can update it before the item is ship...  \n",
      "3        Delivery typically takes 3-5 business days.  \n",
      "4          Yes, we ship to most countries worldwide.  \n"
     ]
    }
   ],
   "source": [
    "# Load and Explore Customer Support Dataset\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'/kaggle/input/customersupportdataset1/sample.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Structured Output (JSON Mode)\n",
    "Configure the language model to return responses in a structured JSON format for better control and integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:11:57.135381Z",
     "iopub.status.busy": "2025-04-17T15:11:57.135110Z",
     "iopub.status.idle": "2025-04-17T15:14:47.666943Z",
     "shell.execute_reply": "2025-04-17T15:14:47.666298Z",
     "shell.execute_reply.started": "2025-04-17T15:11:57.135361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886f021f4b2a4c1695422073bab8aa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231ed174c604424ab693ca425852d419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7574d78183704d05938e4f34b629290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5534596ed99e40feb9c19b35e22cdd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd61cce0e25049429ec46850ca04a303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:12:06.020070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744902726.218366      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744902726.280894      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cd4173f4ad42ce8dd752e213a4a310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee21e55cbe0b4338a1c65399ae946214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b7dc6e27c41af9eae9ec7457a3f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d464976e60e49e98c9a7995d060c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958b1b41030e4fe1a565528febd4a601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725c9324d2c94c2db20377f7a8b95476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Our refund policy varies depending on the product or service. Please refer to our terms and conditions for more information.', 'confidence': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "hf_token= \"104102959865757583751011167382101100107671147079108117856766112120671028489897676107667680\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "\n",
    "def generate_json_response(query):\n",
    "    prompt = f\"\"\"You are a helpful assistant. Respond to the following query in strict JSON format with keys 'response' and 'confidence'.\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Output JSON:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        json_start = result.index('{')\n",
    "        json_str = result[json_start:]\n",
    "        return json.loads(json_str)\n",
    "    except:\n",
    "        return {\"response\": \"Could not parse model output\", \"confidence\": 0.0}\n",
    "\n",
    "query = \"What is your refund policy?\"\n",
    "print(generate_json_response(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting for Query Understanding\n",
    "Provide a few examples of customer queries and their expected responses to fine-tune the model's understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:17:31.286054Z",
     "iopub.status.busy": "2025-04-17T15:17:31.285737Z",
     "iopub.status.idle": "2025-04-17T15:17:37.049050Z",
     "shell.execute_reply": "2025-04-17T15:17:37.048284Z",
     "shell.execute_reply.started": "2025-04-17T15:17:31.286032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a customer support agent. Here are some examples of queries and responses:\n",
      "\n",
      "Query: \"How can I reset my password?\"\n",
      "Response: \"To reset your password, click on 'Forgot Password' on the login page and follow the instructions.\"\n",
      "\n",
      "Query: \"What is your refund policy?\"\n",
      "Response: \"Our refund policy allows returns within 30 days of purchase. Please visit our website for more details.\"\n",
      "\n",
      "Now respond to the following query:\n",
      "Query: \"Can I change my shipping address after placing an order?\"\n",
      "Response:\n",
      "Query: \"Can I change my shipping address after placing an order?\"\n",
      "Response: \"Yes, you can definitely change your shipping address after placing an order. Please visit our website and log into your account. From there, you can navigate to the 'My Orders' section and select the order you want to update. From there, you can change your shipping address and submit the changes. If you have already received your package, please contact our customer service team and they will assist you with the process.\"\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "You are a customer support agent. Here are some examples of queries and responses:\n",
    "\n",
    "Query: \"How can I reset my password?\"\n",
    "Response: \"To reset your password, click on 'Forgot Password' on the login page and follow the instructions.\"\n",
    "\n",
    "Query: \"What is your refund policy?\"\n",
    "Response: \"Our refund policy allows returns within 30 days of purchase. Please visit our website for more details.\"\n",
    "\n",
    "Now respond to the following query:\n",
    "Query: \"Can I change my shipping address after placing an order?\"\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "def generate_few_shot_response(query):\n",
    "    prompt = few_shot_prompt + f'Query: \"{query}\"\\nResponse:'\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Use CPU\n",
    "\n",
    "    \n",
    "    output = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "\n",
    "    \n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "query = \"Can I change my shipping address after placing an order?\"\n",
    "print(generate_few_shot_response(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and Search Customer Queries (Vector Search)\n",
    "Use embeddings to convert customer queries into vectors and perform similarity searches in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:22:10.871791Z",
     "iopub.status.busy": "2025-04-17T15:22:10.871193Z",
     "iopub.status.idle": "2025-04-17T15:22:37.819176Z",
     "shell.execute_reply": "2025-04-17T15:22:37.818558Z",
     "shell.execute_reply.started": "2025-04-17T15:22:10.871766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926065104b5a4e03b253512cdc0b56f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar queries: ['How can I reset my password?', 'Can I change my shipping address after placing an order?', 'What is your refund policy?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "hf_token = \"104102959865757583751011167382101100107671147079108117856766112120671028489897676107667680\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        \n",
    "        embeddings = output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "def embed_and_search(query, query_vector_db):\n",
    "    query_vector = get_embedding(query)\n",
    "    similarities = cosine_similarity(query_vector, query_vector_db)\n",
    "    top_k_idx = np.argsort(similarities[0])[::-1][:5]\n",
    "    return top_k_idx\n",
    "\n",
    "query_texts = [\n",
    "    \"How can I reset my password?\",\n",
    "    \"What is your refund policy?\",\n",
    "    \"Can I change my shipping address after placing an order?\"\n",
    "]\n",
    "query_vector_db = np.vstack([get_embedding(q) for q in query_texts])\n",
    "\n",
    "query = \"How do I change my shipping address?\"\n",
    "search_results = embed_and_search(query, query_vector_db)\n",
    "\n",
    "print(\"Top 5 most similar queries:\", [query_texts[i] for i in search_results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Agent's Performance\n",
    "Evaluate the agent's responses using metrics like accuracy, relevance, and customer satisfaction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:26:20.034355Z",
     "iopub.status.busy": "2025-04-17T15:26:20.034045Z",
     "iopub.status.idle": "2025-04-17T15:26:20.040214Z",
     "shell.execute_reply": "2025-04-17T15:26:20.039440Z",
     "shell.execute_reply.started": "2025-04-17T15:26:20.034334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def evaluate_responses(true_responses, generated_responses, threshold=0.8, verbose=True):\n",
    "    assert len(true_responses) == len(generated_responses), \"Mismatched list lengths\"\n",
    "\n",
    "    def similarity(a, b):\n",
    "        return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "    correct = 0\n",
    "    for i, (true, gen) in enumerate(zip(true_responses, generated_responses)):\n",
    "        sim = similarity(true, gen)\n",
    "        if sim >= threshold:\n",
    "            correct += 1\n",
    "        elif verbose:\n",
    "            print(f\"\\n‚ùå Mismatch at index {i}:\")\n",
    "            print(f\"Expected: {true}\")\n",
    "            print(f\"Generated: {gen}\")\n",
    "            print(f\"Similarity: {sim:.2f}\")\n",
    "\n",
    "    accuracy = correct / len(true_responses)\n",
    "    print(f\"\\n‚úÖ Approximate Accuracy (threshold={threshold}): {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Agent's Response Generation\n",
    "Now we will create a function that will take input from a compatible csv file and will generate the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:34:45.431027Z",
     "iopub.status.busy": "2025-04-17T15:34:45.430708Z",
     "iopub.status.idle": "2025-04-17T15:34:45.442920Z",
     "shell.execute_reply": "2025-04-17T15:34:45.442067Z",
     "shell.execute_reply.started": "2025-04-17T15:34:45.431007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_tests_from_csv(\n",
    "    csv_path,\n",
    "    response_model,\n",
    "    response_tokenizer,\n",
    "    embedding_model,\n",
    "    embedding_tokenizer\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    queries = df[\"query\"].tolist()\n",
    "    true_responses = df[\"true_response\"].tolist()\n",
    "\n",
    "\n",
    "    def generate_json_response(query):\n",
    "        prompt = f\"\"\"You are a helpful assistant. Respond to the following query in strict JSON format with keys 'response' and 'confidence'.\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Output JSON:\"\"\"\n",
    "\n",
    "        inputs = response_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = response_model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)\n",
    "        result = response_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            json_start = result.index('{')\n",
    "            json_str = result[json_start:]\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return {\"response\": \"Could not parse model output\", \"confidence\": 0.0}\n",
    "\n",
    " \n",
    "    def get_embedding(text):\n",
    "        inputs = embedding_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "            if hasattr(outputs, \"last_hidden_state\"):\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            elif hasattr(outputs, \"hidden_states\"):\n",
    "                embeddings = outputs.hidden_states[-1].mean(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"Model output does not contain embeddings.\")\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    " \n",
    "    def embed_and_search(query, query_vector_db):\n",
    "        query_vector = get_embedding(query)\n",
    "        similarities = cosine_similarity(query_vector, query_vector_db)\n",
    "        top_k_idx = np.argsort(similarities[0])[::-1][:5]\n",
    "        return top_k_idx\n",
    "\n",
    "    def evaluate_responses(true_responses, generated_responses):\n",
    "        accuracy = sum([1 for true, gen in zip(true_responses, generated_responses) if true.strip().lower() == gen.strip().lower()]) / len(true_responses)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "    # Run tests\n",
    "    print(\"üîç Testing `generate_json_response`\")\n",
    "    generated_json_responses = []\n",
    "    for q in queries:\n",
    "        json_resp = generate_json_response(q)\n",
    "        generated_json_responses.append(json_resp[\"response\"])\n",
    "        print(f\"Query: {q}\\nGenerated: {json_resp}\\n\")\n",
    "\n",
    "    print(\"\\nüîç Testing `embed_and_search`\")\n",
    "    query_vectors = np.array([get_embedding(q) for q in queries]).squeeze(axis=1)\n",
    "    search_query = \"Can I update my shipping details?\"\n",
    "    top_results = embed_and_search(search_query, query_vectors)\n",
    "    print(f\"Top match indexes for '{search_query}': {top_results}\")\n",
    "\n",
    "    print(\"\\nüîç Testing `evaluate_responses`\")\n",
    "    evaluate_responses(true_responses, generated_json_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cases\n",
    "Here are some example testcase of the functions that are taking input from a csv file: sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T16:06:21.861800Z",
     "iopub.status.busy": "2025-04-17T16:06:21.860969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a96411a59b4e6a9a2184b1e0238f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "response_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", use_auth_token=hf_token)\n",
    "response_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=hf_token)\n",
    "\n",
    "if response_tokenizer.pad_token is None:\n",
    "    response_tokenizer.pad_token = response_tokenizer.eos_token\n",
    "\n",
    "\n",
    "embedding_tokenizer = response_tokenizer \n",
    "embedding_model = response_model \n",
    "\n",
    "run_tests_from_csv(\n",
    "    csv_path=\"/kaggle/input/customersupportdataset1/sample.csv\",\n",
    "    response_model=response_model,\n",
    "    response_tokenizer=response_tokenizer,\n",
    "    embedding_model=embedding_model,\n",
    "    embedding_tokenizer=embedding_tokenizer\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
